{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f660a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b70623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EIN                       34299\n",
       "NAME                      19568\n",
       "APPLICATION_TYPE             17\n",
       "AFFILIATION                   6\n",
       "CLASSIFICATION               71\n",
       "USE_CASE                      5\n",
       "ORGANIZATION                  4\n",
       "STATUS                        2\n",
       "INCOME_AMT                    9\n",
       "SPECIAL_CONSIDERATIONS        2\n",
       "ASK_AMT                    8747\n",
       "IS_SUCCESSFUL                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "application_df = pd.read_csv('Resources/charity_data.csv')\n",
    "application_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37aa52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = application_df.drop(['IS_SUCCESSFUL'], axis=1)\n",
    "y = application_df['IS_SUCCESSFUL']\n",
    "\n",
    "# Apply one-hot encoding to categorical variables\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40fdcdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Model Architecture\n",
    "# Define the model architecture\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# Add hidden layers\n",
    "nn.add(tf.keras.layers.Dense(units=128, activation='relu', input_dim=X_train.shape[1]))\n",
    "nn.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "\n",
    "# Add output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c90269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Training the Model\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler to the training data\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the features data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "409ec73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "804/804 [==============================] - 40s 48ms/step - loss: 0.0884 - accuracy: 0.9633\n",
      "Epoch 2/100\n",
      "804/804 [==============================] - 41s 50ms/step - loss: 0.0882 - accuracy: 0.9632\n",
      "Epoch 3/100\n",
      "804/804 [==============================] - 35s 44ms/step - loss: 0.0879 - accuracy: 0.9637\n",
      "Epoch 4/100\n",
      "804/804 [==============================] - 34s 42ms/step - loss: 0.0874 - accuracy: 0.9641\n",
      "Epoch 5/100\n",
      "804/804 [==============================] - 36s 45ms/step - loss: 0.0868 - accuracy: 0.9640\n",
      "Epoch 6/100\n",
      "804/804 [==============================] - 34s 42ms/step - loss: 0.0866 - accuracy: 0.9644\n",
      "Epoch 7/100\n",
      "804/804 [==============================] - 41s 51ms/step - loss: 0.0859 - accuracy: 0.9649\n",
      "Epoch 8/100\n",
      "804/804 [==============================] - 37s 45ms/step - loss: 0.0857 - accuracy: 0.9648\n",
      "Epoch 9/100\n",
      "804/804 [==============================] - 35s 44ms/step - loss: 0.0854 - accuracy: 0.9650\n",
      "Epoch 10/100\n",
      "804/804 [==============================] - 36s 45ms/step - loss: 0.0847 - accuracy: 0.9652\n",
      "Epoch 11/100\n",
      "804/804 [==============================] - 37s 46ms/step - loss: 0.0839 - accuracy: 0.9655\n",
      "Epoch 12/100\n",
      "804/804 [==============================] - 36s 45ms/step - loss: 0.0837 - accuracy: 0.9653\n",
      "Epoch 13/100\n",
      "804/804 [==============================] - 34s 43ms/step - loss: 0.0833 - accuracy: 0.9651\n",
      "Epoch 14/100\n",
      "804/804 [==============================] - 34s 42ms/step - loss: 0.0828 - accuracy: 0.9657\n",
      "Epoch 15/100\n",
      "804/804 [==============================] - 32s 40ms/step - loss: 0.0824 - accuracy: 0.9659\n",
      "Epoch 16/100\n",
      "804/804 [==============================] - 32s 40ms/step - loss: 0.0816 - accuracy: 0.9667\n",
      "Epoch 17/100\n",
      "804/804 [==============================] - 32s 40ms/step - loss: 0.0815 - accuracy: 0.9664\n",
      "Epoch 18/100\n",
      "804/804 [==============================] - 32s 40ms/step - loss: 0.0814 - accuracy: 0.9668\n",
      "Epoch 19/100\n",
      "804/804 [==============================] - 31s 39ms/step - loss: 0.0809 - accuracy: 0.9668\n",
      "Epoch 20/100\n",
      "804/804 [==============================] - 32s 40ms/step - loss: 0.0801 - accuracy: 0.9670\n",
      "Epoch 21/100\n",
      "804/804 [==============================] - 31s 38ms/step - loss: 0.0804 - accuracy: 0.9672\n",
      "Epoch 22/100\n",
      "804/804 [==============================] - 32s 40ms/step - loss: 0.0798 - accuracy: 0.9671\n",
      "Epoch 23/100\n",
      "804/804 [==============================] - 31s 38ms/step - loss: 0.0796 - accuracy: 0.9674\n",
      "Epoch 24/100\n",
      "804/804 [==============================] - 31s 38ms/step - loss: 0.0789 - accuracy: 0.9671\n",
      "Epoch 25/100\n",
      "804/804 [==============================] - 31s 39ms/step - loss: 0.0788 - accuracy: 0.9677\n",
      "Epoch 26/100\n",
      "804/804 [==============================] - 32s 40ms/step - loss: 0.0782 - accuracy: 0.9679\n",
      "Epoch 27/100\n",
      "804/804 [==============================] - 31s 39ms/step - loss: 0.0784 - accuracy: 0.9675\n",
      "Epoch 28/100\n",
      "804/804 [==============================] - 31s 38ms/step - loss: 0.0777 - accuracy: 0.9678\n",
      "Epoch 29/100\n",
      "804/804 [==============================] - 33s 41ms/step - loss: 0.0775 - accuracy: 0.9682\n",
      "Epoch 30/100\n",
      "804/804 [==============================] - 33s 41ms/step - loss: 0.0771 - accuracy: 0.9684\n",
      "Epoch 31/100\n",
      "804/804 [==============================] - 31s 39ms/step - loss: 0.0767 - accuracy: 0.9684\n",
      "Epoch 32/100\n",
      "804/804 [==============================] - 31s 39ms/step - loss: 0.0767 - accuracy: 0.9686\n",
      "Epoch 33/100\n",
      "804/804 [==============================] - 32s 40ms/step - loss: 0.0762 - accuracy: 0.9691\n",
      "Epoch 34/100\n",
      "804/804 [==============================] - 31s 39ms/step - loss: 0.0762 - accuracy: 0.9691\n",
      "Epoch 35/100\n",
      "804/804 [==============================] - 31s 39ms/step - loss: 0.0758 - accuracy: 0.9686\n",
      "Epoch 36/100\n",
      "804/804 [==============================] - 34s 42ms/step - loss: 0.0758 - accuracy: 0.9693\n",
      "Epoch 37/100\n",
      "804/804 [==============================] - 44s 54ms/step - loss: 0.0753 - accuracy: 0.9692\n",
      "Epoch 38/100\n",
      "804/804 [==============================] - 46s 57ms/step - loss: 0.0752 - accuracy: 0.9689\n",
      "Epoch 39/100\n",
      "804/804 [==============================] - 50s 63ms/step - loss: 0.0749 - accuracy: 0.9697\n",
      "Epoch 40/100\n",
      "804/804 [==============================] - 39s 49ms/step - loss: 0.0749 - accuracy: 0.9693\n",
      "Epoch 41/100\n",
      "804/804 [==============================] - 41s 51ms/step - loss: 0.0750 - accuracy: 0.9691\n",
      "Epoch 42/100\n",
      "804/804 [==============================] - 35s 44ms/step - loss: 0.0748 - accuracy: 0.9691\n",
      "Epoch 43/100\n",
      "804/804 [==============================] - 46s 57ms/step - loss: 0.0742 - accuracy: 0.9700\n",
      "Epoch 44/100\n",
      "804/804 [==============================] - 35s 44ms/step - loss: 0.0744 - accuracy: 0.9693\n",
      "Epoch 45/100\n",
      "804/804 [==============================] - 39s 48ms/step - loss: 0.0740 - accuracy: 0.9703\n",
      "Epoch 46/100\n",
      "804/804 [==============================] - 34s 42ms/step - loss: 0.0736 - accuracy: 0.9706\n",
      "Epoch 47/100\n",
      "804/804 [==============================] - 30s 37ms/step - loss: 0.0736 - accuracy: 0.9697\n",
      "Epoch 48/100\n",
      "804/804 [==============================] - 37s 46ms/step - loss: 0.0734 - accuracy: 0.9703\n",
      "Epoch 49/100\n",
      "804/804 [==============================] - 36s 45ms/step - loss: 0.0741 - accuracy: 0.9696\n",
      "Epoch 50/100\n",
      "804/804 [==============================] - 33s 41ms/step - loss: 0.0751 - accuracy: 0.9699\n",
      "Epoch 51/100\n",
      "804/804 [==============================] - 33s 41ms/step - loss: 0.0730 - accuracy: 0.9702\n",
      "Epoch 52/100\n",
      "804/804 [==============================] - 36s 45ms/step - loss: 0.0728 - accuracy: 0.9706\n",
      "Epoch 53/100\n",
      "804/804 [==============================] - 33s 41ms/step - loss: 0.0725 - accuracy: 0.9703\n",
      "Epoch 54/100\n",
      "804/804 [==============================] - 31s 38ms/step - loss: 0.0727 - accuracy: 0.9702\n",
      "Epoch 55/100\n",
      "804/804 [==============================] - 30s 38ms/step - loss: 0.0727 - accuracy: 0.9701\n",
      "Epoch 56/100\n",
      "804/804 [==============================] - 41s 51ms/step - loss: 0.0720 - accuracy: 0.9700\n",
      "Epoch 57/100\n",
      "804/804 [==============================] - 36s 45ms/step - loss: 0.0725 - accuracy: 0.9699\n",
      "Epoch 58/100\n",
      "804/804 [==============================] - 37s 46ms/step - loss: 0.0720 - accuracy: 0.9705\n",
      "Epoch 59/100\n",
      "804/804 [==============================] - 41s 51ms/step - loss: 0.0717 - accuracy: 0.9709\n",
      "Epoch 60/100\n",
      "804/804 [==============================] - 41s 51ms/step - loss: 0.0719 - accuracy: 0.9703\n",
      "Epoch 61/100\n",
      "804/804 [==============================] - 45s 56ms/step - loss: 0.0719 - accuracy: 0.9702\n",
      "Epoch 62/100\n",
      "804/804 [==============================] - 46s 58ms/step - loss: 0.0715 - accuracy: 0.9704\n",
      "Epoch 63/100\n",
      "804/804 [==============================] - 42s 52ms/step - loss: 0.0715 - accuracy: 0.9710\n",
      "Epoch 64/100\n",
      "804/804 [==============================] - 44s 55ms/step - loss: 0.0715 - accuracy: 0.9705\n",
      "Epoch 65/100\n",
      "804/804 [==============================] - 35s 43ms/step - loss: 0.0710 - accuracy: 0.9706\n",
      "Epoch 66/100\n",
      "804/804 [==============================] - 33s 41ms/step - loss: 0.0708 - accuracy: 0.9704\n",
      "Epoch 67/100\n",
      "804/804 [==============================] - 33s 41ms/step - loss: 0.0711 - accuracy: 0.9703\n",
      "Epoch 68/100\n",
      "804/804 [==============================] - 34s 42ms/step - loss: 0.0709 - accuracy: 0.9712\n",
      "Epoch 69/100\n",
      "804/804 [==============================] - 35s 43ms/step - loss: 0.0705 - accuracy: 0.9711\n",
      "Epoch 70/100\n",
      "804/804 [==============================] - 35s 43ms/step - loss: 0.0705 - accuracy: 0.9710\n",
      "Epoch 71/100\n",
      "804/804 [==============================] - 35s 44ms/step - loss: 0.0708 - accuracy: 0.9709\n",
      "Epoch 72/100\n",
      "804/804 [==============================] - 40s 49ms/step - loss: 0.0698 - accuracy: 0.9710\n",
      "Epoch 73/100\n",
      "804/804 [==============================] - 32s 40ms/step - loss: 0.0716 - accuracy: 0.9709\n",
      "Epoch 74/100\n",
      "804/804 [==============================] - 33s 41ms/step - loss: 0.0706 - accuracy: 0.9706\n",
      "Epoch 75/100\n",
      "804/804 [==============================] - 33s 40ms/step - loss: 0.0697 - accuracy: 0.9711\n",
      "Epoch 76/100\n",
      "804/804 [==============================] - 46s 57ms/step - loss: 0.0696 - accuracy: 0.9709\n",
      "Epoch 77/100\n",
      "804/804 [==============================] - 42s 53ms/step - loss: 0.0693 - accuracy: 0.9714\n",
      "Epoch 78/100\n",
      "804/804 [==============================] - 45s 57ms/step - loss: 0.0698 - accuracy: 0.9710\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804/804 [==============================] - 45s 56ms/step - loss: 0.0694 - accuracy: 0.9707\n",
      "Epoch 80/100\n",
      "804/804 [==============================] - 51s 64ms/step - loss: 0.0694 - accuracy: 0.9715\n",
      "Epoch 81/100\n",
      "804/804 [==============================] - 47s 59ms/step - loss: 0.0693 - accuracy: 0.9714\n",
      "Epoch 82/100\n",
      "804/804 [==============================] - 41s 51ms/step - loss: 0.0694 - accuracy: 0.9715\n",
      "Epoch 83/100\n",
      "804/804 [==============================] - 43s 53ms/step - loss: 0.0694 - accuracy: 0.9712\n",
      "Epoch 84/100\n",
      "804/804 [==============================] - 42s 53ms/step - loss: 0.0695 - accuracy: 0.9713\n",
      "Epoch 85/100\n",
      "804/804 [==============================] - 37s 46ms/step - loss: 0.0689 - accuracy: 0.9716\n",
      "Epoch 86/100\n",
      "804/804 [==============================] - 33s 41ms/step - loss: 0.0708 - accuracy: 0.9715\n",
      "Epoch 87/100\n",
      "804/804 [==============================] - 33s 41ms/step - loss: 0.0760 - accuracy: 0.9714\n",
      "Epoch 88/100\n",
      "804/804 [==============================] - 36s 45ms/step - loss: 0.0784 - accuracy: 0.9712\n",
      "Epoch 89/100\n",
      "804/804 [==============================] - 32s 40ms/step - loss: 0.0689 - accuracy: 0.9713\n",
      "Epoch 90/100\n",
      "804/804 [==============================] - 32s 39ms/step - loss: 0.0682 - accuracy: 0.9717\n",
      "Epoch 91/100\n",
      "804/804 [==============================] - 36s 44ms/step - loss: 0.0683 - accuracy: 0.9711\n",
      "Epoch 92/100\n",
      "804/804 [==============================] - 34s 42ms/step - loss: 0.0683 - accuracy: 0.9719\n",
      "Epoch 93/100\n",
      "804/804 [==============================] - 35s 44ms/step - loss: 0.0685 - accuracy: 0.9711\n",
      "Epoch 94/100\n",
      "804/804 [==============================] - 37s 46ms/step - loss: 0.0682 - accuracy: 0.9713\n",
      "Epoch 95/100\n",
      "804/804 [==============================] - 36s 44ms/step - loss: 0.0684 - accuracy: 0.9719\n",
      "Epoch 96/100\n",
      "804/804 [==============================] - 37s 46ms/step - loss: 0.0680 - accuracy: 0.9718\n",
      "Epoch 97/100\n",
      "804/804 [==============================] - 39s 48ms/step - loss: 0.0685 - accuracy: 0.9712\n",
      "Epoch 98/100\n",
      "804/804 [==============================] - 38s 47ms/step - loss: 0.0687 - accuracy: 0.9712\n",
      "Epoch 99/100\n",
      "804/804 [==============================] - 40s 49ms/step - loss: 0.0686 - accuracy: 0.9715\n",
      "Epoch 100/100\n",
      "804/804 [==============================] - 40s 49ms/step - loss: 0.0682 - accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = nn.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d14970c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 [==============================] - 2s 8ms/step - loss: 1.1440 - accuracy: 0.6588\n",
      "Loss: 1.1440248489379883, Accuracy: 0.6587755084037781\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Evaluate the Model\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657d94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save the Model\n",
    "nn.save('AlphabetSoupCharity_Optimisation2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c908a57f",
   "metadata": {},
   "source": [
    "After evaluating the model using the test data, the results are as follows:\n",
    "\n",
    "The model achieved a loss of 1.1440 and an accuracy of 0.6588. This means that the model correctly predicted the outcome for approximately 65.88% of the test samples.\n",
    "\n",
    "While the accuracy is above the 50% mark, indicating that the model performs better than random guessing, there is still room for improvement.\n",
    "\n",
    "To optimize the model further and aim for a target predictive accuracy higher than 75%, the following steps can be taken:\n",
    "\n",
    "1. Adjust the model architecture: Experiment with adding more hidden layers or increasing the number of neurons in each layer. This allows the model to capture more complex patterns in the data.\n",
    "\n",
    "2. Try different activation functions: The choice of activation functions in the hidden layers can have an impact on the model's performance. Experiment with different activation functions such as 'relu', 'sigmoid', or 'tanh' to see if they improve the accuracy.\n",
    "\n",
    "3. Increase the number of epochs: Training the model for more epochs allows it to learn from the data for a longer period. Gradually increasing the number of epochs and monitoring the model's performance can help identify the optimal training duration.\n",
    "\n",
    "4. Feature engineering: Explore the dataset and consider creating new features or transforming existing ones to better represent the underlying patterns. This could involve binning numerical features, encoding categorical variables differently, or performing other transformations.\n",
    "\n",
    "By implementing these optimization strategies and fine-tuning the model, it is possible to achieve a target predictive accuracy higher than 75%.\n",
    "\n",
    "In summary, the initial model achieved an accuracy of 65.88%, indicating that it performs better than random guessing. However, further optimization is necessary to reach the target accuracy. By adjusting the model architecture, activation functions, and training duration, the model's performance can be improved. Additionally, feature engineering techniques can be employed to enhance the representation of the data. With these improvements, the model can be better equipped to solve the classification problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a080a2",
   "metadata": {},
   "source": [
    "Analysis: Predicting Successful Donations with Deep Learning\n",
    "\n",
    "1. Introduction\n",
    "\n",
    "The purpose of this analysis is to develop a deep learning model that can predict whether a donation request made to Alphabet Soup, a fictional charity organization, will be successful or not. By training the model on historical data, the goal is to achieve a predictive accuracy higher than 75% to assist Alphabet Soup in identifying the most promising donation opportunities.\n",
    "\n",
    "2. Data Preprocessing\n",
    "\n",
    "- Target variable: The target variable for our model is \"IS_SUCCESSFUL,\" which indicates whether a donation request was successful (1) or not (0).\n",
    "- Features: The features used to predict the target variable include variables such as \"APPLICATION_TYPE,\" \"AFFILIATION,\" \"USE_CASE,\" and others.\n",
    "- Excluded variables: Variables that are neither targets nor features, such as unique identifiers or irrelevant metadata, are removed from the input data during preprocessing.\n",
    "\n",
    "3. Compiling, Training, and Evaluating the Model\n",
    "\n",
    "- Model architecture: The neural network model consists of multiple layers, with a varying number of neurons in each layer. Activation functions are used to introduce non-linearity into the model, allowing it to learn complex patterns in the data.\n",
    "- Optimization attempts: Initially, the model achieved an accuracy of 65.88%, which is below the target of 75%. To improve performance, the model architecture, activation functions, and training duration can be adjusted.\n",
    "- Results:\n",
    "  - Loss: The loss value indicates the amount of error the model made during training. Lower values indicate better performance.\n",
    "  - Accuracy: The accuracy metric measures the percentage of correctly predicted outcomes. Higher values indicate better performance.\n",
    "- Summary: The initial model achieved an accuracy of 65.88%, indicating its ability to perform better than random guessing. However, further optimization is required to meet the target accuracy of over 75%.\n",
    "\n",
    "4. Using a Different Model\n",
    "\n",
    "To solve the classification problem of predicting successful donations, an alternative model that could be considered is the Random Forest algorithm. Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. It offers several advantages:\n",
    "\n",
    "- Handling categorical variables: Random Forest can handle categorical variables directly without the need for explicit encoding, making it suitable for datasets with multiple categorical features.\n",
    "- Feature importance: The algorithm provides a measure of feature importance, allowing us to identify the variables that contribute the most to the prediction.\n",
    "- Non-linear relationships: Random Forest can capture non-linear relationships between features and the target variable.\n",
    "- Robustness to outliers: The algorithm is less sensitive to outliers compared to some other models, making it more resilient to noise in the data.\n",
    "\n",
    "By using a Random Forest model, we can leverage its ability to handle categorical variables and capture complex relationships to potentially improve the prediction accuracy for successful donations.\n",
    "\n",
    "In conclusion, the initial deep learning model achieved moderate accuracy but fell short of the target performance. By adjusting the model architecture, activation functions, and training duration, along with exploring alternative models such as Random Forest, we can aim for higher accuracy in predicting successful donations for Alphabet Soup. The insights gained from this analysis can guide the organization in making informed decisions about donation opportunities and maximize their impact in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bd0f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
